# **Ceph的安装与实践**



#### 创建一个Ceph用户

在所有节点上创建一个名为“ cephuser ” 的新用户

 

useradd -d /home/cephuser -m cephuser

 

passwd cephuser



​	![img](/docs/img/x4/图片2.jpg)



 

echo "cephuser ALL = (root) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/cephuser

chmod 0440 /etc/sudoers.d/cephuser

sed -i s'/Defaults requiretty/#Defaults requiretty'/g /etc/sudoers

 





#### 安装和配置NTP

安装NTP以同步所有节点上的日期和时间。运行ntpdate命令通过NTP协议设置日期和时间，我们将使用us pool NTP服务器。然后启动并启用NTP服务器在引导时运行。



 ![](/docs/img/x4/图片3.jpg) 



yum install -y ntp ntpdate ntp-doc

ntpdate 0.us.pool.ntp.org

hwclock --systohc

systemctl enable ntpd.service

systemctl start ntpd.service



#### 安装Open-vm-tools

yum install -y open-vm-tools

#### 禁用SELinux

通过使用sed流编辑器编辑SELinux配置文件，在所有节点上禁用SELinux。

sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config



![img](/docs/img/x4/图片4.jpg) 



#### 网口

编辑文件network-scripts/ifcfg，设置onboot =“ on”，确保网络通信





### **克隆镜像，生成3个新的节点**



![img](/docs/img/x4/图片5.jpg) 



####  节点ip分别为

admin-node 192.168.112.120

nod 1 192.168.112.122

nod 2 192.168.112.124

nod 3 192.168.112.123

 



1.8配置主机文件，在四台虚拟机上，都要编辑/ etc / hosts文件



1.9验证四台虚拟机可以互通



![img](/docs/img/x4/图片6.jpg) 



2.配置SSH服务器



登录cephuser，生成ssh key并修改config文件



![img](/docs/img/x4/图片7.jpg) 



复制ssh ID到其他节点：

ssh-keyscan node1 node2 node3 >> ~/.ssh/known_hosts
ssh-copy-id node1
ssh-copy-id node2
ssh-copy-id node3

尝试从ceph-admin节点访问nod1服务器。

ssh nod1



![img](/docs/img/x4/图片8.jpg) 



## **配置防火墙**

我们将使用防火墙保护系统。在此步骤中，我们将在所有节点上启用firewald，然后打开ceph-admon，ceph-mon和ceph-osd所需的端口。

登录到ceph-admin节点并启动firewalld。

ssh root@ceph-admin

systemctl start firewalld

systemctl enable firewalld



![img](/docs/img/x4/图片9.jpg)  



打开端口80、2003和4505-4506，然后重新加载防火墙。

sudo firewall-cmd --zone=public --add-port=80/tcp --permanent

sudo firewall-cmd --zone=public --add-port=2003/tcp --permanent

sudo firewall-cmd --zone=public --add-port=4505-4506/tcp --permanentsudo firewall-cmd --reload



![img](/docs/img/x4/图片10.jpg) 



ssh nod1

sudo systemctl start firewalld

sudo systemctl enable firewalld



![img](/docs/img/x4/图片11.jpg) 



最后，打开每个osd节点上的端口6800-7300-osd1，osd2和os3。

从ceph-admin节点登录到每个osd节点。

ssh osd1

sudo systemctl start firewalld

sudo systemctl enable firewalld

打开端口并重新加载防火墙。

sudo firewall-cmd --zone=public --add-port=6800-7300/tcp --permanent

sudo firewall-cmd --reload

![img](/docs/img/x4/图片12.jpg) 

## **配置Ceph OSD节点**



在ceph-admin节点上安装ceph-deploy

 

sudo rpm -Uhv http://download.ceph.com/rpm-jewel/el7/noarch/ceph-release-1-1.el7.noarch.rpm



![img](/docs/img/x4/图片13.jpg)



sudo yum update -y && sudo yum install ceph-deploy -y

![img](/docs/img/x4/图片14.jpg)









### **创建新的集群配置**

创建新的群集目录。

mkdir cluster

cd cluster/

使用“ **ceph** **-deploy** ”命令创建一个新的集群配置，将监视节点定义为“ **nod1** ”。

ceph-deploy new nod1

在[global]块下，在下面粘贴配置。

\# Your network address

public network = 10.0.15.0/24

osd pool default size = 2



![img](/docs/img/x4/图片15.jpg)



![img](/docs/img/x4/图片16.jpg)



### **在所有节点上安装Ceph**

ceph-deploy install  node1 node2 node 3

将ceph-mon部署在node1节点上

ceph-deploy mon create-initial



![img](/docs/img/x4/图片17.jpg)



ceph-deploy gatherkeys nod1



![img](/docs/img/x4/图片18.jpg)





